1)Decision Tree

import pandas
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
%matplotlib inline
df=pandas.read_csv(r"C:\Users\Student\Desktop\dataSet.csv")
d={'UK':0,'USA':1,'N':2}
df['Nationality']=df['Nationality'].map(d)
d={'YES':1,'NO':0}
df['Go']=df['Go'].map(d)
features=['Age','Experience','Rank','Nationality']
X=df[features]
y=df['Go']
dtree=DecisionTreeClassifier(criterion='entropy')
dtree=dtree.fit(X.values,y)
tree.plot_tree(dtree,feature_names=features)
print(dtree.predict([[40,10,7,1]]))

2)KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import random
data_iris=load_iris()
label_target=data_iris.target_names
print()
print("Sample data from the iris dataset: ")
print("*"*30)
for i in range(10):
 rn=random.randint(0,120)
 print(data_iris.data[rn],"==>",label_target[data_iris.target[rn]])
X=data_iris.data
y=data_iris.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_s
tate=8)
print("The training dataset length: ",len(X_train))
print("The testing dataset length: ",len(X_test))
try:
 nn=int(input("Enter number of neighbors: "))
 knn=KNeighborsClassifier(nn)
 knn.fit(X_train,y_train)
 test_data=input("Enter test data: ").split(",")
 for i in range(len(test_data)):
 test_data[i]=float(test_data[i])
 print()
 v=knn.predict([test_data])
 print("Predicted output is: ", label_target[v])
except:
 print("Please provide valid input...")

3)Linear Regression

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
np.random.seed(0)
X=2*np.random.rand(100,1)
y=4+3*X + np.random.randn(100,1)
model=LinearRegression()
model.fit(X,y)
X_new=np.array([[0],[2]])
y_predict=model.predict(X_new)
plt.plot(X_new,y_predict,"r-",linewidth=2,label="Predictions")
plt.plot(X,y,"b.",label="Data points")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",rotation=0,fontsize=18)
plt.legend()
plt.axis([0,2,0,15])
plt.show()
print("Intercept: ",model.intercept_)
print("Coefficient: ", model.coef_)

4)Logistic Regression

import warnings
warnings.filterwarnings("ignore")
import numpy
from sklearn import linear_model
%matplotlib inline
X=numpy.array([3.78,2.44,2.09,0.14,1.72,1.65,4.92,4.37,4.96,4.52,3.69,5
.88]).reshape(-1,1)
y=numpy.array([0,0,0,0,0,0,1,1,1,1,1,1])
logr=linear_model.LogisticRegression()
logr.fit(X,y)
predicted=logr.predict(numpy.array([3.46]).reshape(-1,1))
print(predicted)

5)Naive Bayes

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn import metrics
data={
 'outlook': 
['sunny','sunny','overcast','rainy','rainy','rainy','overcast','sunny','sunny','r
ainy','sunny','overcast','overcast','rainy'],
 'temp': 
['hot','hot','hot','mild','cool','cool','cool','mild','cool','mild','mild','mild','h
ot','mild'],
 'humidity': 
['high','high','high','high','normal','normal','normal','high','normal','norm
al','normal','high','normal','high'],
 'windy': 
['false','true','false','false','false','true','true','false','false','false','true','tru
e','false','true'],
 'play': 
['no','no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no']
}
df=pd.DataFrame(data)
df
df_c=df.astype('category')
df_c["outlook"]=df_c["outlook"].cat.codes
df_c["temp"]=df_c["temp"].cat.codes
df_c["humidity"]=df_c["humidity"].cat.codes
df_c["windy"]=df_c["windy"].cat.codes
df_c["play"]=df_c["play"].cat.codes
df_c.head(14)
X=df_c.iloc[:,:4].values
X
Y=df_c.iloc[:,4].values
Y
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_s
tate=45)
print(X_train.shape,y_train.shape)
model=GaussianNB()
model.fit(X_train,y_train)
predicted=model.predict([[0,2,2,1]])
print("Predicted value: ",predicted)

6)Synthetic Classification Dataset

from sklearn.datasets import make_classification
X,y=make_classification(n_samples=10,n_features=5,n_informative=2,n_
redundant=2,random_state=1)
print(X.shape,y.shape)
print(X)

7)Bagging

from sklearn.datasets import make_classification
from sklearn.ensemble import BaggingClassifier
X,y=make_classification(n_samples=10,n_features=5,n_informative=2,n_
redundant=2,random_state=1)
model=BaggingClassifier()
model.fit(X,y)
row=[[1.65980218,-1.04052679,0.89368622,1.03584131,-1.55118469]]
yhat=model.predict(row)
print('Predicted class: %d' %yhat)

8)Boosting

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
X,y=make_classification(n_samples=10,n_features=5,n_informative=2,n_
redundant=2,random_state=1)
cl=GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_
depth=1)
cl.fit(X,y)
print(X.shape,y.shape)
print(X)
row=[[0.191,1.054,-0.729,-1.146,1.446]]
yhat=cl.predict(row)
print('Predicted class: %d' %yhat[0])

9)Random Forest

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import 
RandomizedSearchCV,train_test_split
bank_data=pd.read_csv(r"bank.csv")
bank_data=bank_data.loc[:,['age','default','cons.price.idx','cons.conf.idx',
'y']]
bank_data.head(5)
bank_data['default']=bank_data['default'].map({'no':0,'yes':1,'unknown':
0})
bank_data['y']=bank_data['y'].map({'no':0,'yes':1})
X=bank_data.drop('y',axis=1)
y=bank_data['y']
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
rf=RandomForestClassifier()
rf.fit(X_train,y_train)
row=[[1,57,0,93.994]]
y_pred=rf.predict(row)
print('Prediction: ',y_pred)

10)Fuzzy C means

import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl
np.random.seed(0)
data=np.random.rand(100,2)
n_clusters=3
cntr,u,u0,d,j,m,p=fuzz.cluster.cmeans(data.T,n_clusters,2,error=0.005,ma
xiter=1000,init=None)
cluster_membership=np.argmax(u,axis=0)
print('Cluster centers: ',cntr)
print('Cluster membership: ',cluster_membership)

11)Kmeans
p1:
from sklearn.cluster import KMeans
import numpy as np
import warnings
warnings.filterwarnings('ignore')
X=np.array([[1.7,1.5],[0.1,0.7],[0.35,1.24],[0.94,1.56],[1.26,1.106],[1.54,0
.41],[0.45,1.79],[0.77,0.18]])
y=np.array([0,1,1,0,1,0,1,1,1])
kmeans=KMeans(n_clusters=2,random_state=0).fit(X)
output=kmeans.predict([[1.71,1.58]])
print(output)

p2:
#KMeans with custom dataset
import pandas as pd
from sklearn.cluster import KMeans
import numpy as np
import warnings
warnings.filterwarnings('ignore')
df=pd.read_csv("Height-Weight.csv")
X=df[["Height","Weight"]]
kmeans=KMeans(n_clusters=2,random_state=0).fit(X)
output=kmeans.predict([[171,56]])
print(output)

12)Confusion Matrix

import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import 
confusion_matrix,precision_score,recall_score,f1_score,accuracy_score
import matplotlib.pyplot as plt
bc=datasets.load_breast_cancer()
X=bc.data
y=bc.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_s
tate=1,stratify=y)
sc=StandardScaler()
sc.fit(X_train)
X_train_std=sc.transform(X_train)
X_test_std=sc.transform(X_test)
svc=SVC(kernel='linear',random_state=1)
svc.fit(X_train_std,y_train)
y_pred=svc.predict(X_test_std)
conf_matrix=confusion_matrix(y_true=y_test,y_pred=y_pred)
fig,ax=plt.subplots(figsize=(5,5))
ax.matshow(conf_matrix,cmap=plt.cm.Oranges,alpha=0.3)
for i in range(conf_matrix.shape[0]):
 for j in range(conf_matrix.shape[1]):
 ax.text(x=j,y=i,s=conf_matrix[i,j],va='center',ha='center',size='xxlarge')
plt.xlabel('Predictions',fontsize=18)
plt.ylabel('Actuals',fontsize=18)
plt.title('Confusion matrix',fontsize=18)
plt.show()
print('Precision: %3f'%precision_score(y_test,y_pred))
print('Recall: %3f'%recall_score(y_test,y_pred))
print('Accuracy: %3f'%accuracy_score(y_test,y_pred))
print('F1 Score: %3f'%f1_score(y_test,y_pred))

13)Agglomerative Clustering

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram,linkage
x=[4,5,10,4,3,11,14,6,10,12]
y=[21,19,24,17,16,25,24,22,21,21]
data=list(zip(x,y))
hierarchical_cluster=AgglomerativeClustering(n_clusters=2,metric='eucli
dean',linkage='ward')
labels=hierarchical_cluster.fit_predict(data)
plt.scatter(x,y,c=labels)
plt.show()
linkage_data=linkage(data,method='ward',metric='euclidean')
dendrogram(linkage_data)
plt.show()
print(labels)

14)Text Classification 

import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn import metrics
import nltk
from nltk.corpus import stopwords
import string
nltk.download('stopwords')
data=fetch_20newsgroups(subset='all')
def preprocess(text):
 text=''.join([char for char in text if char not in string.punctuation])
 tokens=text.split()
 tokens=[word for word in tokens if word.lower() not in 
stopwords.words('english')]
 return ''.join(tokens)
data.data=[preprocess(text) for text in data.data]
X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_
size=0.2,random_state=42)
model=make_pipeline(TfidfVectorizer(),SVC(kernel='linear'))
model.fit(X_train,y_train)
predicted=model.predict(X_test)
accuracy=metrics.accuracy_score(y_test,predicted)
print(f'Accuracy: {accuracy}')
print(metrics.classification_report(y_test,predicted,target_names=data.t
arget_names))


